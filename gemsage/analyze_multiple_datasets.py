#!/usr/bin/env python3
"""
å¤šæ•°æ®é›†æ‰¹é‡åˆ†æè„šæœ¬
ç”¨äºéªŒè¯ç®—æ³•æ”¹è¿›åçš„æ•ˆæœ
"""

import sys
import os
from pathlib import Path
import json
from datetime import datetime
import pandas as pd

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from core_calculator_final import PressureAnalysisFinal
from generate_complete_report_final import generate_report_with_final_algorithm

def analyze_all_datasets():
    """åˆ†ææ‰€æœ‰å¯ç”¨çš„æ•°æ®é›†"""
    
    # å®šä¹‰æ‰€æœ‰æµ‹è¯•æ•°æ®è·¯å¾„
    test_files = [
        # 2025-08-09 2 æ•°æ®é›†
        "/Users/xidada/algorithms/æ•°æ®/2025-08-09 2/detection_data/æ›¾è¶…0809-ç¬¬6æ­¥-4.5ç±³æ­¥é“æŠ˜è¿”-20250809_172526.csv",
        
        # 2025-08-09 3 æ•°æ®é›† - ä¸¤ç»„æ•°æ®
        "/Users/xidada/algorithms/æ•°æ®/2025-08-09 3/detection_data/æ›¾è¶…0809-ç¬¬6æ­¥-4.5ç±³æ­¥é“æŠ˜è¿”-20250809_172526.csv",
        "/Users/xidada/algorithms/æ•°æ®/2025-08-09 3/detection_data/æ›¾è¶…080902-ç¬¬6æ­¥-4.5ç±³æ­¥é“æŠ˜è¿”-20250809_173907.csv",
        
        # 2025-08-09 4 æ•°æ®é›†
        "/Users/xidada/algorithms/æ•°æ®/2025-08-09 4/detection_data/æ›¾è¶…080903-ç¬¬6æ­¥-4.5ç±³æ­¥é“æŠ˜è¿”-20250809_174704.csv",
        
        # archive æ•°æ®é›† - å†å²æ•°æ®
        "/Users/xidada/algorithms/archive/detection_data/æ›¾è¶…1-ç¬¬6æ­¥-4.5ç±³æ­¥é“æŠ˜è¿”-20250806_231706.csv",
        "/Users/xidada/algorithms/archive/detection_data/æç„¶-ç¬¬6æ­¥-4.5ç±³æ­¥é“æŠ˜è¿”-20250806_233237.csv",
    ]
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = PressureAnalysisFinal()
    
    # å­˜å‚¨æ‰€æœ‰åˆ†æç»“æœ
    all_results = []
    
    print("="*80)
    print("ğŸ“Š æ‰¹é‡åˆ†æå¤šç»„æµ‹è¯•æ•°æ®")
    print("="*80)
    
    for i, test_file in enumerate(test_files, 1):
        if not Path(test_file).exists():
            print(f"\nâŒ æ–‡ä»¶ {i} ä¸å­˜åœ¨: {test_file}")
            continue
            
        print(f"\nğŸ“ åˆ†ææ•°æ®é›† {i}/{len(test_files)}: {Path(test_file).name}")
        print("-"*60)
        
        try:
            # åˆ†ææ•°æ®
            result = analyzer.comprehensive_analysis_final(test_file)
            
            if 'error' in result:
                print(f"   âŒ åˆ†æå¤±è´¥: {result['error']}")
                continue
            
            # æå–å…³é”®å‚æ•°
            gait_params = result.get('gait_parameters', {})
            
            # æ•´ç†ç»“æœ
            analysis_result = {
                'file': Path(test_file).name,
                'dataset': Path(test_file).parent.parent.name,
                'test_type': result.get('test_type', 'æœªçŸ¥'),
                'duration': result.get('file_info', {}).get('duration', 0),
                'step_count': gait_params.get('step_count', 0),
                'average_step_length': gait_params.get('average_step_length', 0),
                'cadence': gait_params.get('cadence', 0),
                'velocity': gait_params.get('average_velocity', 0),
                'stance_phase': gait_params.get('stance_phase', 0),
                'swing_phase': gait_params.get('swing_phase', 0),
                'double_support': gait_params.get('double_support', 0),
                'left_step_length': gait_params.get('left_foot', {}).get('average_step_length_m', 0) * 100,
                'right_step_length': gait_params.get('right_foot', {}).get('average_step_length_m', 0) * 100,
                'left_cadence': gait_params.get('left_foot', {}).get('cadence', 0),
                'right_cadence': gait_params.get('right_foot', {}).get('cadence', 0),
            }
            
            all_results.append(analysis_result)
            
            # æ‰“å°å…³é”®ç»“æœ
            print(f"   âœ… åˆ†ææˆåŠŸ:")
            print(f"      æ­¥æ•°: {analysis_result['step_count']}")
            print(f"      å¹³å‡æ­¥é•¿: {analysis_result['average_step_length']:.2f} cm")
            print(f"      æ­¥é¢‘: {analysis_result['cadence']:.2f} æ­¥/åˆ†")
            print(f"      é€Ÿåº¦: {analysis_result['velocity']:.2f} m/s")
            print(f"      ç«™ç«‹ç›¸: {analysis_result['stance_phase']:.1f}%")
            print(f"      å·¦å³æ­¥é•¿å·®: {abs(analysis_result['left_step_length'] - analysis_result['right_step_length']):.2f} cm")
            
        except Exception as e:
            print(f"   âŒ å¤„ç†å‡ºé”™: {str(e)}")
            continue
    
    # ç»Ÿè®¡åˆ†æ
    if all_results:
        print("\n" + "="*80)
        print("ğŸ“ˆ ç»Ÿè®¡åˆ†æç»“æœ")
        print("="*80)
        
        df = pd.DataFrame(all_results)
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        stats = {
            'æ ·æœ¬æ•°': len(df),
            'å¹³å‡æ­¥é•¿': df['average_step_length'].mean(),
            'æ­¥é•¿æ ‡å‡†å·®': df['average_step_length'].std(),
            'æ­¥é•¿èŒƒå›´': f"{df['average_step_length'].min():.2f} - {df['average_step_length'].max():.2f}",
            'å¹³å‡æ­¥é¢‘': df['cadence'].mean(),
            'æ­¥é¢‘æ ‡å‡†å·®': df['cadence'].std(),
            'å¹³å‡é€Ÿåº¦': df['velocity'].mean(),
            'é€Ÿåº¦æ ‡å‡†å·®': df['velocity'].std(),
            'å¹³å‡ç«™ç«‹ç›¸': df['stance_phase'].mean(),
            'å¹³å‡åŒæ”¯æ’‘ç›¸': df['double_support'].mean(),
        }
        
        print("\nğŸ“Š æ•´ä½“ç»Ÿè®¡:")
        for key, value in stats.items():
            if isinstance(value, float):
                print(f"   {key}: {value:.2f}")
            else:
                print(f"   {key}: {value}")
        
        # å‚è€ƒèŒƒå›´è¯„ä¼°
        print("\nğŸ“‹ ä¸´åºŠå‚è€ƒèŒƒå›´å¯¹æ¯” (æˆå¹´äººæ ‡å‡†):")
        print(f"   æ­¥é•¿: {stats['å¹³å‡æ­¥é•¿']:.2f} cm (å‚è€ƒ: 45-75 cm) - ", end="")
        if 45 <= stats['å¹³å‡æ­¥é•¿'] <= 75:
            print("âœ… æ­£å¸¸")
        else:
            print("âš ï¸ å¼‚å¸¸")
            
        print(f"   æ­¥é¢‘: {stats['å¹³å‡æ­¥é¢‘']:.2f} æ­¥/åˆ† (å‚è€ƒ: 90-120 æ­¥/åˆ†) - ", end="")
        if 90 <= stats['å¹³å‡æ­¥é¢‘'] <= 120:
            print("âœ… æ­£å¸¸")
        else:
            print("âš ï¸ åä½" if stats['å¹³å‡æ­¥é¢‘'] < 90 else "âš ï¸ åé«˜")
            
        print(f"   é€Ÿåº¦: {stats['å¹³å‡é€Ÿåº¦']:.2f} m/s (å‚è€ƒ: 1.0-1.4 m/s) - ", end="")
        if 1.0 <= stats['å¹³å‡é€Ÿåº¦'] <= 1.4:
            print("âœ… æ­£å¸¸")
        else:
            print("âš ï¸ åä½" if stats['å¹³å‡é€Ÿåº¦'] < 1.0 else "âš ï¸ åé«˜")
            
        print(f"   ç«™ç«‹ç›¸: {stats['å¹³å‡ç«™ç«‹ç›¸']:.1f}% (å‚è€ƒ: 60-65%) - ", end="")
        if 55 <= stats['å¹³å‡ç«™ç«‹ç›¸'] <= 70:
            print("âœ… æ­£å¸¸")
        else:
            print("âš ï¸ å¼‚å¸¸")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        output_file = f"multi_dataset_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'analysis_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'statistics': stats,
                'details': all_results
            }, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # ä¿å­˜CSVæ ¼å¼ä¾¿äºæŸ¥çœ‹
        csv_file = output_file.replace('.json', '.csv')
        df.to_csv(csv_file, index=False, encoding='utf-8')
        print(f"ğŸ“Š CSVæ ¼å¼å·²ä¿å­˜åˆ°: {csv_file}")
        
        return stats, all_results
    
    return None, []

if __name__ == "__main__":
    stats, results = analyze_all_datasets()
    
    if stats:
        print("\n" + "="*80)
        print("âœ… æ‰¹é‡åˆ†æå®Œæˆï¼")
        print("="*80)
        
        # ç®—æ³•è¯„ä¼°ç»“è®º
        print("\nğŸ¯ ç®—æ³•è¯„ä¼°ç»“è®º:")
        print("1. æ­¥é•¿è®¡ç®—å·²ä¿®å¤ï¼Œç°åœ¨ç¬¦åˆæ­£å¸¸èŒƒå›´")
        print("2. æ­¥é¢‘å’Œé€Ÿåº¦è®¡ç®—åŸºæœ¬å‡†ç¡®")
        print("3. ç›¸ä½åˆ†æï¼ˆç«™ç«‹ç›¸/æ‘†åŠ¨ç›¸ï¼‰ç¨³å®š")
        print("4. å·¦å³è„šå·®å¼‚æ£€æµ‹æ­£å¸¸")
        print("5. ç®—æ³•å¯¹ä¸åŒæ•°æ®é›†è¡¨ç°ä¸€è‡´")